{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "462de08a",
   "metadata": {},
   "source": [
    "## User-Defined Constants\n",
    "\n",
    "Set these before each run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aaad362",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION=\"us-central1\"\n",
    "PROJECT_ID = \"rax-datascience-dev\"\n",
    "DATASET_NAME = \"inventory_forecasting\"\n",
    "RUN_DATE = \"2022-07-01\"\n",
    "# Note: Use underscores instead of dashes for BQ compatibility\n",
    "DISPLAY_NAME_PREFIX = \"automl_inventory\"\n",
    "MAPE_THRESHOLD = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8821e90",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "These are the libraries for pipeline creation.  *NOT* libraries for individual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "530c7952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from dateutil import parser\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, Metrics, component)\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7d86b",
   "metadata": {},
   "source": [
    "## Calculated Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05a7ed4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n",
      "\n",
      "\n",
      "\n",
      "USER-DEFINED CONSTANTS\n",
      "***************************\n",
      "REGION: us-central1\n",
      "PROJECT_ID: rax-datascience-dev\n",
      "DATASET_NAME: inventory_forecasting\n",
      "RUN_DATE: 2022-07-01\n",
      "DISPLAY_NAME_PREFIX: automl_inventory\n",
      "MAPE_THRESHOLD: 35\n",
      "\n",
      "\n",
      "\n",
      "CALCULATED CONSTANTS\n",
      "***************************\n",
      "PATH: /usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n",
      "BUCKET_NAME: gs://rax-datascience-dev-inventory-forecasting-bucket\n",
      "PIPELINE_ROOT: gs://rax-datascience-dev-inventory-forecasting-bucket/pipeline_root/\n",
      "API_ENDPOINT: us-central1-aiplatform.googleapis.com\n",
      "RUN_DATE_TIMESTAMP: 2022-07-01 00:00:00\n",
      "RUN_DATE_STRIPPED: 20220701\n",
      "RUN_DATE_DASHES: 2022-07-01\n",
      "DISPLAY_NAME: automl_inventory_20220701\n",
      "DISPLAY_NAME_ONLINE: automl_inventory_online_20220701\n",
      "DISPLAY_NAME_OFFLINE: automl_inventory_offline_20220701\n",
      "THRESHOLDS_DICT: {\"meanAbsolutePercentageError\": 35}\n",
      "BQ_PROJ_DATASET: rax-datascience-dev.inventory_forecasting\n",
      "BQ_PROJ_DATASET_URI: bq://rax-datascience-dev.inventory_forecasting\n",
      "BQ_SOURCE_FEATURES: bq://rax-datascience-dev.inventory_forecasting.forecasting_model_features_vertex\n",
      "BQ_SOURCE_BATCH: bq://rax-datascience-dev.inventory_forecasting.forecasting_model_batch_vertex\n",
      "BQ_SOURCE_EVALUATED_ONLINE: bq://rax-datascience-dev.inventory_forecasting.evaluate_data_items_automl_inventory_online_20220701\n",
      "BQ_SOURCE_EVALUATED_OFFLINE: bq://rax-datascience-dev.inventory_forecasting.evaluate_data_items_automl_inventory_offline_20220701\n",
      "QUERY_CREATE_FEATURES: CALL `rax-datascience-dev.inventory_forecasting.udsp_forecasting_master`(DATE('2022-07-01'))\n",
      "QUERY_COMBINE_RESULTS: CALL `rax-datascience-dev.inventory_forecasting.udsp_forecasting_combine_results`(DATE('2022-07-01'))\n"
     ]
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "BUCKET_NAME=\"gs://\" + PROJECT_ID + \"-inventory-forecasting-bucket\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\n",
    "API_ENDPOINT = REGION + \"-aiplatform.googleapis.com\"\n",
    "\n",
    "RUN_DATE_TIMESTAMP = parser.parse(RUN_DATE)\n",
    "RUN_DATE_STRIPPED = RUN_DATE_TIMESTAMP.strftime(\"%Y%m%d\")\n",
    "RUN_DATE_DASHES = RUN_DATE_TIMESTAMP.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Note: Use underscores instead of dashes for BQ compatibility\n",
    "DISPLAY_NAME = DISPLAY_NAME_PREFIX + \"_{}\".format(RUN_DATE_STRIPPED)\n",
    "DISPLAY_NAME_ONLINE = DISPLAY_NAME_PREFIX + \"_online_{}\".format(RUN_DATE_STRIPPED)\n",
    "DISPLAY_NAME_OFFLINE = DISPLAY_NAME_PREFIX + \"_offline_{}\".format(RUN_DATE_STRIPPED)\n",
    "\n",
    "THRESHOLDS_DICT = \"{\\\"meanAbsolutePercentageError\\\": \" + str(MAPE_THRESHOLD) + \"}\"\n",
    "\n",
    "BQ_PROJ_DATASET = PROJECT_ID + \".\" + DATASET_NAME\n",
    "BQ_PROJ_DATASET_URI = \"bq://\" + BQ_PROJ_DATASET \n",
    "BQ_SOURCE_FEATURES = BQ_PROJ_DATASET_URI + \".forecasting_model_features_vertex\"\n",
    "BQ_SOURCE_BATCH = BQ_PROJ_DATASET_URI + \".forecasting_model_batch_vertex\"\n",
    "BQ_SOURCE_EVALUATED_ONLINE = BQ_PROJ_DATASET_URI + \".evaluate_data_items_\" + DISPLAY_NAME_ONLINE\n",
    "BQ_SOURCE_EVALUATED_OFFLINE = BQ_PROJ_DATASET_URI + \".evaluate_data_items_\" + DISPLAY_NAME_OFFLINE\n",
    "\n",
    "QUERY_CREATE_FEATURES = \"CALL `\" + BQ_PROJ_DATASET + \".udsp_forecasting_master`(DATE('\" + RUN_DATE_DASHES + \"'))\"\n",
    "QUERY_COMBINE_RESULTS = \"CALL `\" + BQ_PROJ_DATASET + \".udsp_forecasting_combine_results`(DATE('\" + RUN_DATE_DASHES + \"'))\"\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"USER-DEFINED CONSTANTS\")\n",
    "print(\"***************************\")\n",
    "print(\"REGION:\", REGION)\n",
    "print(\"PROJECT_ID:\", PROJECT_ID)\n",
    "print(\"DATASET_NAME:\", DATASET_NAME)\n",
    "print(\"RUN_DATE:\", RUN_DATE)\n",
    "print(\"DISPLAY_NAME_PREFIX:\", DISPLAY_NAME_PREFIX)\n",
    "print(\"MAPE_THRESHOLD:\", MAPE_THRESHOLD)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"CALCULATED CONSTANTS\")\n",
    "print(\"***************************\")\n",
    "print(\"PATH:\", PATH)\n",
    "print(\"BUCKET_NAME:\", BUCKET_NAME)\n",
    "print(\"PIPELINE_ROOT:\", PIPELINE_ROOT)\n",
    "print(\"API_ENDPOINT:\", API_ENDPOINT)\n",
    "print(\"RUN_DATE_TIMESTAMP:\", RUN_DATE_TIMESTAMP)\n",
    "print(\"RUN_DATE_STRIPPED:\", RUN_DATE_STRIPPED)\n",
    "print(\"RUN_DATE_DASHES:\", RUN_DATE_DASHES)\n",
    "print(\"DISPLAY_NAME:\", DISPLAY_NAME)\n",
    "print(\"DISPLAY_NAME_ONLINE:\", DISPLAY_NAME_ONLINE)\n",
    "print(\"DISPLAY_NAME_OFFLINE:\", DISPLAY_NAME_OFFLINE)\n",
    "print(\"THRESHOLDS_DICT:\", THRESHOLDS_DICT)\n",
    "print(\"BQ_PROJ_DATASET:\", BQ_PROJ_DATASET)\n",
    "print(\"BQ_PROJ_DATASET_URI:\", BQ_PROJ_DATASET_URI)\n",
    "print(\"BQ_SOURCE_FEATURES:\", BQ_SOURCE_FEATURES)\n",
    "print(\"BQ_SOURCE_BATCH:\", BQ_SOURCE_BATCH)\n",
    "print(\"BQ_SOURCE_EVALUATED_ONLINE:\", BQ_SOURCE_EVALUATED_ONLINE)\n",
    "print(\"BQ_SOURCE_EVALUATED_OFFLINE:\", BQ_SOURCE_EVALUATED_OFFLINE)\n",
    "print(\"QUERY_CREATE_FEATURES:\", QUERY_CREATE_FEATURES)\n",
    "print(\"QUERY_COMBINE_RESULTS:\", QUERY_COMBINE_RESULTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b25bb9",
   "metadata": {},
   "source": [
    "## Custom Module to Execute SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08673c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\",\n",
    "    output_component_file=\"sql_query_component.yaml\", # Optional: you can use this to load the component later\n",
    "    packages_to_install=[\"google-cloud-bigquery\"],\n",
    ")\n",
    "def run_bigquery_ddl(\n",
    "    project: str, \n",
    "    location: str,\n",
    "    query_string: str, \n",
    ") -> NamedTuple('Outputs', [('created_table', str), ('query', str)]):\n",
    "    \"\"\"\n",
    "    Runs BigQuery query and returns a table/model name\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    from google.api_core.future import polling\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud.bigquery import retry as bq_retry\n",
    "    \n",
    "    print(query_string)\n",
    "        \n",
    "    bqclient = bigquery.Client(project=project, location=location)\n",
    "    job = bqclient.query(query_string, retry=bq_retry.DEFAULT_RETRY)\n",
    "    job._retry = polling.DEFAULT_RETRY\n",
    "    \n",
    "    while job.running():\n",
    "        from time import sleep\n",
    "        sleep(0.1)\n",
    "        print('Running ...')\n",
    "    \n",
    "    log_msg = 'JOB completed in {}'.format(job.ended - job.started)\n",
    "    print(log_msg)\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    result_tuple = namedtuple('DDLOutput', ['created_table', 'query'])\n",
    "    return result_tuple(\"\", query_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1735f",
   "metadata": {},
   "source": [
    "## Custom Module for Sending Email Notifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca20def7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\",\n",
    "    output_component_file=\"send_gmail_component.yaml\", # Optional: you can use this to load the component later\n",
    "    packages_to_install=[\"google-api-python-client\"],\n",
    ")\n",
    "def send_email_operator(\n",
    "    to_string: str,\n",
    "    from_string: str,\n",
    "    subject: str,\n",
    "    message_text: str,\n",
    ") -> NamedTuple('Outputs', [('message_id', str), ('raw', str)]):\n",
    "    \"\"\"\n",
    "    Sends an email via the service accounts credentials using gmail api.\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    import base64\n",
    "    from urllib.error import HTTPError\n",
    "    from email.mime.text import MIMEText\n",
    "    \n",
    "    import google.auth.transport.requests\n",
    "    from googleapiclient.discovery import build\n",
    "    import google.auth\n",
    "\n",
    "    # Note - we must set the appropriate scope for the default service account\n",
    "    SCOPES = [\"https://www.googleapis.com/auth/gmail.send\"]\n",
    "    credentials, project = google.auth.default(\n",
    "        scopes=SCOPES\n",
    "    )\n",
    "    \n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    credentials.refresh(auth_req)\n",
    "\n",
    "    \n",
    "    def create_message(sender, to, subject, message_text):\n",
    "        \"\"\"Create a message for an email.\n",
    "\n",
    "        Args:\n",
    "            sender: Email address of the sender.\n",
    "            to: Email address of the receiver.\n",
    "            subject: The subject of the email message.\n",
    "            message_text: The text of the email message.\n",
    "\n",
    "        Returns:\n",
    "            An object containing a base64url encoded email object.\n",
    "        \"\"\"\n",
    "        message = MIMEText(message_text)\n",
    "        message['to'] = to\n",
    "        message['from'] = sender\n",
    "        message['subject'] = subject\n",
    "        message['raw'] = base64.urlsafe_b64encode(message.as_string().encode('utf-8'))\n",
    "        return {'raw': message['raw'].decode('utf-8')}\n",
    "    \n",
    "    def send_message(service, user_id, message):\n",
    "        \"\"\"Send an email message.\n",
    "\n",
    "        Args:\n",
    "            service: Authorized Gmail API service instance.\n",
    "            user_id: User's email address. The special value \"me\"\n",
    "            can be used to indicate the authenticated user.\n",
    "            message: Message to be sent.\n",
    "\n",
    "        Returns:\n",
    "            Sent Message.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            message = (service.users().messages().send(userId=user_id, body=message)\n",
    "                   .execute())\n",
    "            print('Message Id: %s' % message['id'])\n",
    "            return message\n",
    "        except HTTPError as error:\n",
    "            print('An error occurred: %s' % error)\n",
    "    \n",
    "    # Instantiate the service\n",
    "    service = build('gmail', 'v1', credentials=credentials)\n",
    "    message = create_message(from_string, to_string, subject, message_text)\n",
    "    sent = send_message(service, from_string, message)\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    result_tuple = namedtuple('Output', ['message_id', 'raw'])\n",
    "    return result_tuple(sent['id'], sent['raw'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9095c8e",
   "metadata": {},
   "source": [
    "## Custom Module for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ea17176",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\",\n",
    "    output_component_file=\"forecast_eval_component.yaml\", # Optional: you can use this to load the component later\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    ")\n",
    "def forecast_model_eval_metrics(\n",
    "    project: str,\n",
    "    location: str,  # \"us-central1\",\n",
    "    api_endpoint: str,  # \"us-central1-aiplatform.googleapis.com\",\n",
    "    thresholds_dict_str: str,\n",
    "    model: Input[Model],\n",
    "    metrics: Output[Metrics],    \n",
    ") -> NamedTuple(\"Outputs\", [(\"dep_decision\", str)]):  # Return parameter.\n",
    "\n",
    "    \"\"\"This function renders evaluation metrics for an AutoML Tabular forecasting model.\n",
    "    It retrieves the classification model evaluation generated by the AutoML Tabular forecasting\n",
    "    training process, does some parsing, and uses that info to render the MAPE calculation\n",
    "    for the model. It also uses given metrics threshold information and compares that to the\n",
    "    evaluation results to determine whether the model is sufficiently accurate to deploy.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import logging\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    from google.protobuf.json_format import MessageToDict\n",
    "    \n",
    "    # Fetch model eval info\n",
    "    def get_eval_info(client, model_name):\n",
    "        \n",
    "\n",
    "        response = client.list_model_evaluations(parent=model_name)\n",
    "        metrics_list = []\n",
    "        metrics_string_list = []\n",
    "        for evaluation in response:\n",
    "            print(\"model_evaluation\")\n",
    "            print(\" name:\", evaluation.name)\n",
    "            print(\" metrics_schema_uri:\", evaluation.metrics_schema_uri)\n",
    "            metrics = MessageToDict(evaluation._pb.metrics)\n",
    "            for metric in metrics.keys():\n",
    "                logging.info(\"metric: %s, value: %s\", metric, metrics[metric])\n",
    "            metrics_str = json.dumps(metrics)\n",
    "            metrics_list.append(metrics)\n",
    "            metrics_string_list.append(metrics_str)\n",
    "\n",
    "        return (\n",
    "            evaluation.name,\n",
    "            metrics_list,\n",
    "            metrics_string_list,\n",
    "        )\n",
    "\n",
    "    # Use the given metrics threshold(s) to determine whether the model is \n",
    "    # accurate enough to deploy.\n",
    "    def classification_thresholds_check(metrics_dict, thresholds_dict):\n",
    "        for k, v in thresholds_dict.items():\n",
    "            logging.info(\"k {}, v {}\".format(k, v))\n",
    "            if k in [\"meanAbsolutePercentageError\"]:  # lower is better\n",
    "                if metrics_dict[k] > v:  # if over threshold, don't deploy\n",
    "                    logging.info(\n",
    "                        \"{} > {}; returning False\".format(metrics_dict[k], v)\n",
    "                    )\n",
    "                    return False\n",
    "        logging.info(\"threshold checks passed.\")\n",
    "        return True\n",
    "\n",
    "    def log_metrics(metrics_list):\n",
    "        mape = metrics_list[0][\"meanAbsolutePercentageError\"]\n",
    "        logging.info(\"mape: %s\", mape)\n",
    "\n",
    "        # log textual metrics info as well\n",
    "        for metric in metrics_list[0].keys():\n",
    "            val_string = json.dumps(metrics_list[0][metric])\n",
    "            metrics.log_metric(metric, val_string)\n",
    "        # metrics.metadata[\"model_type\"] = \"AutoML Tabular classification\"\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    aiplatform.init(project=project)\n",
    "    # extract the model resource name from the input Model Artifact\n",
    "    model_resource_path = model.uri.replace(\"aiplatform://v1/\", \"\")\n",
    "    logging.info(\"model path: %s\", model_resource_path)\n",
    "\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\n",
    "    eval_name, metrics_list, metrics_str_list = get_eval_info(\n",
    "        client, model_resource_path\n",
    "    )\n",
    "    logging.info(\"got evaluation name: %s\", eval_name)\n",
    "    logging.info(\"got metrics list: %s\", metrics_list)\n",
    "    log_metrics(metrics_list)\n",
    "\n",
    "    thresholds_dict = json.loads(thresholds_dict_str)\n",
    "    deploy = classification_thresholds_check(metrics_list[0], thresholds_dict)\n",
    "    if deploy:\n",
    "        dep_decision = \"true\"\n",
    "    else:\n",
    "        dep_decision = \"false\"\n",
    "    logging.info(\"deployment decision is %s\", dep_decision)\n",
    "\n",
    "    return (dep_decision,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada421f5",
   "metadata": {},
   "source": [
    "## Pre-built component configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f1b4328",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"automl-inventory-forecasting\",\n",
    "                  pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    bq_source_features: str = BQ_SOURCE_FEATURES,\n",
    "    bq_source_batch: str = BQ_SOURCE_BATCH,\n",
    "    display_name: str = DISPLAY_NAME,\n",
    "    display_name_online: str = DISPLAY_NAME_ONLINE,\n",
    "    display_name_offline: str = DISPLAY_NAME_OFFLINE,\n",
    "    project: str = PROJECT_ID,\n",
    "    gcp_region: str = REGION,\n",
    "    api_endpoint: str = API_ENDPOINT,\n",
    "    thresholds_dict_str: str = THRESHOLDS_DICT,\n",
    "    query_create_features: str = QUERY_CREATE_FEATURES,\n",
    "    query_combine_results: str = QUERY_COMBINE_RESULTS,\n",
    "    bq_source_eval_online: str = BQ_SOURCE_EVALUATED_ONLINE,\n",
    "    bq_source_eval_offline: str = BQ_SOURCE_EVALUATED_OFFLINE,\n",
    "    bq_source_batch_destination: str = BQ_PROJ_DATASET_URI,\n",
    "):\n",
    "    # Step 1 - Create the tables needed for the dataset from the raw data.  \n",
    "    create_tables_op = run_bigquery_ddl(\n",
    "        project, \n",
    "        gcp_region,\n",
    "        query_create_features, \n",
    "    )\n",
    "    \n",
    "    # Step 2 - Create a Vertex AI Dataset\n",
    "    dataset_create_op = gcc_aip.TimeSeriesDatasetCreateOp(\n",
    "        project=project, \n",
    "        display_name=display_name, \n",
    "        bq_source=bq_source_features,\n",
    "        labels={'foo':'{}'.format(create_tables_op.outputs[\"created_table\"])},\n",
    "    )\n",
    "\n",
    "    # Step 3a - Train the Online Model\n",
    "    online_training_op = gcc_aip.AutoMLForecastingTrainingJobRunOp(\n",
    "        project=project,\n",
    "        display_name=display_name_online,\n",
    "        budget_milli_node_hours=8000,\n",
    "        column_transformations=[\n",
    "            {\"timestamp\": {\"column_name\": \"TMK_TIMESTAMP\"}},\n",
    "            {\"categorical\": {\"column_name\": \"item_status\"}},\n",
    "            {\"numeric\": {\"column_name\": \"full_lead_time\"}},\n",
    "            {\"numeric\": {\"column_name\": \"min\"}},\n",
    "            {\"numeric\": {\"column_name\": \"max\"}},\n",
    "            {\"numeric\": {\"column_name\": \"excess_threshold\"}},\n",
    "            {\"categorical\": {\"column_name\": \"lmc\"}},\n",
    "            {\"numeric\": {\"column_name\": \"dr\"}},\n",
    "            {\"numeric\": {\"column_name\": \"device_count\"}},\n",
    "            {\"numeric\": {\"column_name\": \"opp_count\"}},\n",
    "            {\"numeric\": {\"column_name\": \"online\"}},\n",
    "            {\"numeric\": {\"column_name\": \"offline\"}},\n",
    "        ],\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        target_column=\"online\",\n",
    "        time_column=\"TMK_TIMESTAMP\",\n",
    "        time_series_identifier_column=\"SERIES_IDENTIFIER\",\n",
    "        unavailable_at_forecast_columns = [\n",
    "            \"item_status\",\n",
    "            \"full_lead_time\",\n",
    "            \"min\",\n",
    "            \"max\",\n",
    "            \"excess_threshold\",\n",
    "            \"lmc\",\n",
    "            \"dr\",\n",
    "            \"device_count\",\n",
    "            \"opp_count\",\n",
    "            \"online\",\n",
    "            \"offline\",\n",
    "        ],\n",
    "        available_at_forecast_columns = [\n",
    "            \"TMK_TIMESTAMP\"\n",
    "        ],\n",
    "        forecast_horizon = 6,\n",
    "        data_granularity_unit = \"month\",\n",
    "        data_granularity_count = 1,\n",
    "        context_window = 12,\n",
    "        export_evaluated_data_items = True,\n",
    "        export_evaluated_data_items_bigquery_destination_uri = bq_source_eval_online,\n",
    "        optimization_objective = \"minimize-wape-mae\",\n",
    "    )\n",
    "    \n",
    "    # Step 4a - Evaluate the model metrics\n",
    "    online_model_eval_op = forecast_model_eval_metrics(\n",
    "        project,\n",
    "        gcp_region,\n",
    "        api_endpoint,\n",
    "        thresholds_dict_str,\n",
    "        online_training_op.outputs[\"model\"],\n",
    "    )\n",
    "\n",
    "    # Step 5a - IF the model metrics pass\n",
    "    with dsl.Condition(\n",
    "        online_model_eval_op.outputs[\"dep_decision\"] == \"true\",\n",
    "        name=\"deploy_decision\",\n",
    "    ):\n",
    "        # Step 6a - Batch Prediction for Online\n",
    "        batch_op_online = gcc_aip.ModelBatchPredictOp(\n",
    "            model=online_training_op.outputs[\"model\"],\n",
    "            project=project,\n",
    "            location=gcp_region,\n",
    "            job_display_name=display_name_online,\n",
    "            bigquery_source=bq_source_batch,\n",
    "            bigquery_destination_prefix=bq_source_batch_destination,\n",
    "            predictions_format=\"bigquery\",\n",
    "            #gcs_destination_prefix=BUCKET_NAME,\n",
    "            #predictions_format=\"csv\",\n",
    "        )\n",
    "    \n",
    "    # Step 3b - Train the Offline model\n",
    "    offline_training_op = gcc_aip.AutoMLForecastingTrainingJobRunOp(\n",
    "        project=project,\n",
    "        display_name=display_name_offline,\n",
    "        budget_milli_node_hours=8000,\n",
    "        column_transformations=[\n",
    "            {\"timestamp\": {\"column_name\": \"TMK_TIMESTAMP\"}},\n",
    "            {\"categorical\": {\"column_name\": \"item_status\"}},\n",
    "            {\"numeric\": {\"column_name\": \"full_lead_time\"}},\n",
    "            {\"numeric\": {\"column_name\": \"min\"}},\n",
    "            {\"numeric\": {\"column_name\": \"max\"}},\n",
    "            {\"numeric\": {\"column_name\": \"excess_threshold\"}},\n",
    "            {\"categorical\": {\"column_name\": \"lmc\"}},\n",
    "            {\"numeric\": {\"column_name\": \"dr\"}},\n",
    "            {\"numeric\": {\"column_name\": \"device_count\"}},\n",
    "            {\"numeric\": {\"column_name\": \"opp_count\"}},\n",
    "            {\"numeric\": {\"column_name\": \"online\"}},\n",
    "            {\"numeric\": {\"column_name\": \"offline\"}},\n",
    "        ],\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        target_column=\"offline\",\n",
    "        time_column=\"TMK_TIMESTAMP\",\n",
    "        time_series_identifier_column=\"SERIES_IDENTIFIER\",\n",
    "        unavailable_at_forecast_columns = [\n",
    "            \"item_status\",\n",
    "            \"full_lead_time\",\n",
    "            \"min\",\n",
    "            \"max\",\n",
    "            \"excess_threshold\",\n",
    "            \"lmc\",\n",
    "            \"dr\",\n",
    "            \"device_count\",\n",
    "            \"opp_count\",\n",
    "            \"online\",\n",
    "            \"offline\",\n",
    "        ],\n",
    "        available_at_forecast_columns = [\n",
    "            \"TMK_TIMESTAMP\"\n",
    "        ],\n",
    "        forecast_horizon = 6,\n",
    "        data_granularity_unit = \"month\",\n",
    "        data_granularity_count = 1,\n",
    "        context_window = 12,\n",
    "        export_evaluated_data_items = True,\n",
    "        export_evaluated_data_items_bigquery_destination_uri = bq_source_eval_offline,\n",
    "        optimization_objective = \"minimize-wape-mae\",\n",
    "    )\n",
    "    \n",
    "    # Step 4b - Evaluate the model\n",
    "    offline_model_eval_op = forecast_model_eval_metrics(\n",
    "        project,\n",
    "        gcp_region,\n",
    "        api_endpoint,\n",
    "        thresholds_dict_str,\n",
    "        offline_training_op.outputs[\"model\"],\n",
    "    )\n",
    "\n",
    "    # Step 5b - IF the model eval passes\n",
    "    with dsl.Condition(\n",
    "        offline_model_eval_op.outputs[\"dep_decision\"] == \"true\",\n",
    "        name=\"deploy_decision\",\n",
    "    ):\n",
    "        # Step 6b - Batch Prediction for Offline\n",
    "        batch_op_offline = gcc_aip.ModelBatchPredictOp(\n",
    "            model=offline_training_op.outputs[\"model\"],\n",
    "            project=project,\n",
    "            location=gcp_region,\n",
    "            job_display_name=display_name_offline,\n",
    "            bigquery_source=bq_source_batch,\n",
    "            bigquery_destination_prefix=bq_source_batch_destination,\n",
    "            predictions_format=\"bigquery\",\n",
    "            #gcs_destination_prefix=BUCKET_NAME,\n",
    "            #predictions_format=\"csv\",\n",
    "        )\n",
    "        \n",
    "    # Step 6 - Combine the results\n",
    "    combine_results_op = run_bigquery_ddl(\n",
    "        project, \n",
    "        gcp_region,\n",
    "        query_combine_results, \n",
    "        \n",
    "    ).after(batch_op_online).after(batch_op_offline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedace74",
   "metadata": {},
   "source": [
    "## Compile and Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798cb9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/40046533665/locations/us-central1/pipelineJobs/automl-inventory-forecasting-20220809153909\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/40046533665/locations/us-central1/pipelineJobs/automl-inventory-forecasting-20220809153909')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/automl-inventory-forecasting-20220809153909?project=40046533665\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40046533665/locations/us-central1/pipelineJobs/automl-inventory-forecasting-20220809153909 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40046533665/locations/us-central1/pipelineJobs/automl-inventory-forecasting-20220809153909 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40046533665/locations/us-central1/pipelineJobs/automl-inventory-forecasting-20220809153909 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40046533665/locations/us-central1/pipelineJobs/automl-inventory-forecasting-20220809153909 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40046533665/locations/us-central1/pipelineJobs/automl-inventory-forecasting-20220809153909 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40046533665/locations/us-central1/pipelineJobs/automl-inventory-forecasting-20220809153909 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40046533665/locations/us-central1/pipelineJobs/automl-inventory-forecasting-20220809153909 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40046533665/locations/us-central1/pipelineJobs/automl-inventory-forecasting-20220809153909 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40046533665/locations/us-central1/pipelineJobs/automl-inventory-forecasting-20220809153909 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40046533665/locations/us-central1/pipelineJobs/automl-inventory-forecasting-20220809153909 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40046533665/locations/us-central1/pipelineJobs/automl-inventory-forecasting-20220809153909 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40046533665/locations/us-central1/pipelineJobs/automl-inventory-forecasting-20220809153909 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"tab_forecast_pipeline.json\"\n",
    ")\n",
    "\n",
    "ml_pipeline_job = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"automl-inventory-forecasting\",\n",
    "    template_path=\"tab_forecast_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        \"bq_source_features\": BQ_SOURCE_FEATURES,\n",
    "        \"bq_source_batch\": BQ_SOURCE_BATCH,\n",
    "        \"display_name\": DISPLAY_NAME,\n",
    "        \"display_name_online\": DISPLAY_NAME_ONLINE,\n",
    "        \"display_name_offline\": DISPLAY_NAME_OFFLINE,\n",
    "        \"project\": PROJECT_ID,\n",
    "        \"gcp_region\": REGION,\n",
    "        \"api_endpoint\": API_ENDPOINT,\n",
    "        \"thresholds_dict_str\": THRESHOLDS_DICT,\n",
    "        \"query_create_features\": QUERY_CREATE_FEATURES,\n",
    "        \"query_combine_results\": QUERY_COMBINE_RESULTS,\n",
    "        \"bq_source_eval_online\": BQ_SOURCE_EVALUATED_ONLINE,\n",
    "        \"bq_source_eval_offline\": BQ_SOURCE_EVALUATED_OFFLINE,\n",
    "        \"bq_source_batch_destination\": BQ_PROJ_DATASET_URI,\n",
    "    },\n",
    "    #enable_caching=True,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "ml_pipeline_job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f213a3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
